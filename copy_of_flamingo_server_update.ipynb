{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMagicshNEJXsAlSdFeyuRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shauny123/byword-intake-api/blob/main/copy_of_flamingo_server_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG3aEoLojiYV"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install -q flask flask-cors uvicorn[standard] transformers \\\n",
        "               google-api-python-client google-auth-httplib2 google-auth-oauthlib \\\n",
        "               portpicker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55c21d97",
        "outputId": "2759d0e9-47df-4eb5-d306-633e7bdc90f7"
      },
      "source": [
        "%%writefile flamingo_server.py\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from transformers import pipeline\n",
        "import uvicorn\n",
        "from uvicorn.config import Config\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "try:\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "    logger.info(\"Sentiment analysis model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading sentiment analysis model: {e}\")\n",
        "    sentiment_pipeline = None\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all origins\n",
        "\n",
        "@app.route('/analyze_sentiment', methods=['POST'])\n",
        "def analyze_sentiment():\n",
        "    if sentiment_pipeline is None:\n",
        "        return jsonify({\"error\": \"Sentiment analysis model not loaded\"}), 500\n",
        "\n",
        "    data = request.get_json()\n",
        "    if not data or 'text' not in data:\n",
        "        return jsonify({\"error\": \"Invalid input, please provide 'text'\"}), 400\n",
        "\n",
        "    text = data['text']\n",
        "    try:\n",
        "        result = sentiment_pipeline(text)\n",
        "        logger.info(f\"Sentiment analysis result for '{text}': {result}\")\n",
        "        return jsonify(result[0])\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during sentiment analysis: {e}\")\n",
        "        return jsonify({\"error\": \"Error analyzing sentiment\"}), 500\n",
        "\n",
        "# Use portpicker to find an available port\n",
        "try:\n",
        "    import portpicker\n",
        "    PORT = portpicker.pick_unused_port()\n",
        "    logger.info(f\"Picked unused port: {PORT}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error picking port, using default 8000: {e}\")\n",
        "    PORT = 8000\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"Flamingo Server is running!\"\n",
        "\n",
        "class Server(uvicorn.Server):\n",
        "    \"\"\"Custom uvicorn.Server subclass.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self._startup_event = asyncio.Event()\n",
        "\n",
        "    async def startup(self, sockets=None):\n",
        "        await super().startup(sockets=sockets)\n",
        "        self._startup_event.set()\n",
        "\n",
        "    def run(self, sockets=None):\n",
        "        asyncio.run(self.serve(sockets=sockets))\n",
        "\n",
        "    async def serve(self, sockets=None):\n",
        "        config = self.config\n",
        "        if not config.loaded:\n",
        "            config.load()\n",
        "        self.lifespan = config.lifespan_class(config)\n",
        "        self.install_signal_handlers()\n",
        "        await self.startup(sockets=sockets)\n",
        "        if self.should_exit:\n",
        "            return\n",
        "        await self.main_loop()\n",
        "        await self.shutdown(sockets=sockets)\n",
        "\n",
        "    async def main_loop(self):\n",
        "        while not self.should_exit:\n",
        "            await asyncio.sleep(0.1) # Keep the loop alive\n",
        "\n",
        "    async def shutdown(self, sockets=None):\n",
        "        await self.lifespan.shutdown()\n",
        "\n",
        "    async def serve(self, sockets=None):\n",
        "        await self._startup_event.wait()\n",
        "        await super().serve(sockets=sockets)\n",
        "\n",
        "def run_server():\n",
        "    config = Config(app=app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
        "    server = Server(config=config)\n",
        "    server.run()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Start the server in a separate thread or process if needed,\n",
        "    # but for simplicity in Colab, we'll run it directly.\n",
        "    # In a real application, consider using multiprocessing or threading\n",
        "    # or running with `uvicorn flamingo_server:app --host 0.0.0.0 --port 8000`\n",
        "    logger.info(\"Starting Flamingo Server...\")\n",
        "    run_server()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing flamingo_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8778499",
        "outputId": "7ba07e72-b804-489f-f790-33122c966f1f"
      },
      "source": [
        "!python flamingo_server.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-21 06:00:55.694360: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753077655.723057    2010 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753077655.732103    2010 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-21 06:00:55.761679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "config.json: 100% 629/629 [00:00<00:00, 2.54MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:02<00:00, 96.0MB/s]\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 217kB/s]\n",
            "vocab.txt: 232kB [00:00, 30.7MB/s]\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "flamingo_server.py â€“ GPU-optimized legal-intake backend for Google Colab\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. Colab one-time installs (quiet)\n",
        "# -----------------------------------------------------------\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    import IPython, portpicker, nest_asyncio, uvicorn, threading\n",
        "    IPython.get_ipython().system(\"\"\"\n",
        "        pip install --quiet \\\n",
        "            torch transformers flask flask-cors flask-socketio openai \\\n",
        "            google-api-python-client google-auth-httplib2 google-auth-oauthlib \\\n",
        "            uvicorn[standard] portpicker nest-asyncio\n",
        "    \"\"\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. Standard library\n",
        "# -----------------------------------------------------------\n",
        "import os, json, time, logging, threading, smtplib, torch\n",
        "from datetime import datetime\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. 3rd-party\n",
        "# -----------------------------------------------------------\n",
        "import numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from flask_socketio import SocketIO, emit\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import openai\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4. Logging\n",
        "# -----------------------------------------------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 5. Configuration (no secrets in code)\n",
        "# -----------------------------------------------------------\n",
        "class FlamingoConfig:\n",
        "    HF_TOKEN          = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "    OPENAI_API_KEY    = os.getenv(\"OPENAI_API_KEY\")\n",
        "    EMAIL_USER        = os.getenv(\"EMAIL_USER\", \"bywordofmouthcatering@gmail.com\")\n",
        "    EMAIL_PASSWORD    = os.getenv(\"EMAIL_PASSWORD\", \"\")\n",
        "    USE_GPU           = torch.cuda.is_available()\n",
        "    MODEL_NAME        = \"microsoft/DialoGPT-medium\" if USE_GPU else \"microsoft/DialoGPT-small\"\n",
        "    MAX_LENGTH        = 512 if USE_GPU else 256\n",
        "    DEVICE            = \"cuda\" if USE_GPU else \"cpu\"\n",
        "    SERVICE_ACCOUNT_FILE = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", \"credentials.json\")\n",
        "    SCOPES = [\"https://www.googleapis.com/auth/drive.readonly\"]\n",
        "\n",
        "cfg = FlamingoConfig()\n",
        "openai.api_key = cfg.OPENAI_API_KEY\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 6. Optional Google Drive service\n",
        "# -----------------------------------------------------------\n",
        "drive_service = None\n",
        "if os.path.isfile(cfg.SERVICE_ACCOUNT_FILE):\n",
        "    try:\n",
        "        from google.oauth2 import service_account\n",
        "        from googleapiclient.discovery import build\n",
        "\n",
        "        creds = service_account.Credentials.from_service_account_file(\n",
        "            cfg.SERVICE_ACCOUNT_FILE, scopes=cfg.SCOPES\n",
        "        )\n",
        "        drive_service = build(\"drive\", \"v3\", credentials=creds)\n",
        "        logger.info(\"Google Drive service ready\")\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Google Drive init failed: %s\", e)\n",
        "else:\n",
        "    logger.warning(\"No credentials.json â€“ Drive API disabled\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 7. Load HuggingFace model once\n",
        "# -----------------------------------------------------------\n",
        "logger.info(\"Loading %s on %s\", cfg.MODEL_NAME, cfg.DEVICE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    cfg.MODEL_NAME,\n",
        "    use_auth_token=cfg.HF_TOKEN,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.MODEL_NAME,\n",
        "    use_auth_token=cfg.HF_TOKEN,\n",
        "    trust_remote_code=True,\n",
        ").to(cfg.DEVICE)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "logger.info(\"Model loaded\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 8. Flask + SocketIO app\n",
        "# -----------------------------------------------------------\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "socketio = SocketIO(app, cors_allowed_origins=\"*\")\n",
        "\n",
        "@app.route(\"/health\")\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"ok\", \"gpu\": cfg.USE_GPU, \"model\": cfg.MODEL_NAME})\n",
        "\n",
        "@app.route(\"/chat\", methods=[\"POST\"])\n",
        "def chat():\n",
        "    data = request.get_json(force=True, silent=True) or {}\n",
        "    prompt = data.get(\"prompt\", \"\")\n",
        "    if not prompt:\n",
        "        return jsonify({\"error\": \"prompt required\"}), 400\n",
        "\n",
        "    inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\").to(\n",
        "        cfg.DEVICE\n",
        "    )\n",
        "    reply_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=inputs.shape[1] + cfg.MAX_LENGTH,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    reply = tokenizer.decode(\n",
        "        reply_ids[:, inputs.shape[-1] :][0], skip_special_tokens=True\n",
        "    )\n",
        "    return jsonify({\"reply\": reply.strip()})\n",
        "\n",
        "@socketio.on(\"connect\")\n",
        "def on_connect(auth=None):\n",
        "    emit(\"status\", {\"msg\": \"connected\"})\n",
        "\n",
        "@socketio.on(\"chat\")\n",
        "def on_chat(json_msg):\n",
        "    prompt = json_msg.get(\"prompt\", \"\")\n",
        "    if not prompt:\n",
        "        emit(\"reply\", {\"error\": \"prompt required\"})\n",
        "        return\n",
        "\n",
        "    inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\").to(\n",
        "        cfg.DEVICE\n",
        "    )\n",
        "    reply_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=inputs.shape[1] + cfg.MAX_LENGTH,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    reply = tokenizer.decode(\n",
        "        reply_ids[:, inputs.shape[-1] :][0], skip_special_tokens=True\n",
        "    )\n",
        "    emit(\"reply\", {\"prompt\": prompt, \"reply\": reply.strip()})\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 9. Non-blocking launch (Colab)\n",
        "# -----------------------------------------------------------\n",
        "if __name__ == \"__main__\" and IN_COLAB:\n",
        "    nest_asyncio.apply()  # allow uvicorn inside Jupyter\n",
        "    port = portpicker.pick_unused_port()\n",
        "    logger.info(\"Starting uvicorn on port %d\", port)\n",
        "\n",
        "    def run():\n",
        "        uvicorn.run(\n",
        "            \"flamingo_server:app\",\n",
        "            host=\"0.0.0.0\",\n",
        "            port=port,\n",
        "            log_level=\"info\",\n",
        "            reload=False,\n",
        "        )\n",
        "\n",
        "    threading.Thread(target=run, daemon=True).start()\n",
        "    print(f\"ðŸš€ Ready â€“ server should be reachable at https://localhost:{port}/health\")"
      ],
      "metadata": {
        "id": "TF4awS71oL3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}